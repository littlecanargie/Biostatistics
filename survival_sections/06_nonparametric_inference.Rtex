\subsection{Kaplan-Meier estimator and Greenwood's formula}
Previously we have shown that as long as the censoring is non-informative, we can write the likelihood of the data $\{(y_i, \delta_i)\}_{i=1}^n$ as:
\begin{equation}
    L(\theta_T) = \prod_{i=1}^n f_T(y_i; \theta_T)^{\delta_i}S_T(y_i; \theta_T)^{1-\delta_i}
\end{equation}
where $\theta_T$ is the vector of parameters for the survival distribution we assumed for the event.  Now suppose instead we do not want to make parametric assumptions, so that now we have the likelihood
\begin{equation}
    L(S_T) = \prod_{i=1}^n f_T(y_i)^{\delta_i}S_T(y_i)^{1-\delta_i}
\end{equation}
and our goal is to find a survival distribution function $S_T$ among \textit{all possible survival functions} that maximize the likelihood.  Since we are obtaining our estimator non-parametrically, the MLE for this problem, $\hat{S}_T$, is called the \textit{non-parametric maximum likelihood estimator (NPMLE)}. 

Intuitively, to maximize the likelihood, $\hat{S}_T$ would be a step function only dropping at timepoints where events occur.  To see this, first we define some notations: let $v_1, v_2, ..., v_m$ be distinct ordered timepoints from the set of observations $y_1, y_2, ..., y_n$, and suppose at time $v_j$, $d_j$ subjects experienced events and $c_j$ subjects were censored.  Also by convention we let $v_0 = 0, d_0 = 0, c_0 = 0$ and $v_{m+1} = \infty, d_{m+1} = 0, c_{m+1} = 0$.  With the new notations, the likeihood $L(S_T)$ can be rewritten as
\begin{equation}
    L(S_T) = \prod_{j=0}^{m+1} f_T(v_j)^{d_j}S_T(v_j)^{c_j}
\end{equation}
Now suppose $S_T$ is \textit{any} viable survival function.  Given $k, l \in \{0, 1, 2,...,m\}$ where $k < l$, we now try to improve $S_T$ to $\Tilde{S}_T$ within $t \in [v_k, v_l)$ by maximizing $L(\Tilde{S}_T)$.  Since $\Tilde{S}_T$ is held equal to $S_T$ outside of $[v_k, v_l)$, we only have to maximize the part of the likelihood related to the value of $\Tilde{S}_T$ within $[v_k, v_l)$, which is 
\begin{equation}
    L^{kl}(\Tilde{S}_T) = \Big[\prod_{j=k}^{l-1}\Tilde{f}_T(v_j)^{d_j}\Tilde{S}_T(v_j)^{c_j}\Big]\Tilde{f}_T(v_l)^{d_l}
\end{equation}

First let us set $k,l$ so that there are events at both ends of $[v_k, v_l)$ and no events within $[v_k, v_l)$, i.e. $(d_k \ne 0, d_l \ne 0) \wedge (d_j = 0, \forall k < j < l)$.  Under this setting, we have the partial likelihood
\begin{equation}
    L^{kl}(\Tilde{S}_T) = \Tilde{f}_T(v_k)^{d_k}\Big[\prod_{j=k}^{l-1}\Tilde{S}_T(v_j)^{c_j}\Big]\Tilde{f}_T(v_l)^{d_l}
\end{equation}
This partial likelihood implies that \underline{$\Tilde{S}_T$ cannot drop down within the eventless interval $(v_k, v_l)$}, i.e. $\Tilde{S}_T(t) = \Tilde{S}_T(v_k), \; \forall t \in (v_k, v_l)$.  To see this, suppose the condition is violated so that $\Tilde{S}_T(v_l^-) < \Tilde{S}_T(v_k)$.  We can thus define another survival function $\Tilde{S}_T$ by
\begin{align}
    \Tilde{S}^*_T(t) = \Tilde{S}_T(v_k)&, \quad t \in (v_k, v_l)\\
    \Tilde{S}^*_T(t) = \Tilde{S}_T(t)&, \quad \text{otherwise}
\end{align}
Based on the definition, we have the following equalities and inequalities, where $k < j < l$:
\begin{gather}
    \Tilde{S}^*_T(v_k) = \Tilde{S}_T(v_k)\\
    \Tilde{f}^*_T(v_k) = \Tilde{S}^*_T(v_k^-) - \Tilde{S}^*_T(v_k) = \Tilde{S}_T(v_k^-) - \Tilde{S}_T(v_k) = \Tilde{f}_T(v_k)\\
    \Tilde{S}^*_T(v_j) = \Tilde{S}_T(v_k) \ge \Tilde{S}_T(v_j)\\
    \Tilde{f}^*_T(v_l) = \Tilde{S}^*_T(v_l^-) - \Tilde{S}^*_T(v_l) = \Tilde{S}_T(v_k) - \Tilde{S}_T(v_l) > \Tilde{S}_T(v_l^-) - \Tilde{S}_T(v_l) =  \Tilde{f}_T(v_l)
\end{gather}
Therefore, we have
\begin{equation}
    L^{kl}(\Tilde{S}_T^*) = \Tilde{f}^*_T(v_k)^{d_k}\Big[\prod_{j=k}^{l-1}\Tilde{S}^*_T(v_j)^{c_j}\Big]\Tilde{f}^*_T(v_l)^{d_l} > \Tilde{f}_T(v_k)^{d_k}\Big[\prod_{j=k}^{l-1}\Tilde{S}_T(v_j)^{c_j}\Big]\Tilde{f}_T(v_l)^{d_l} = L^{kl}(\Tilde{S}_T)
\end{equation}
which contradicts with the fact that $\Tilde{S}_T$ maximizes the partial likelihood $L^{kl}$.  

\medskip
Second, let us set $k = 0$ and let $l$ be the first timepoint with event occurrence, i.e. $(d_l \ne 0) \wedge (d_j = 0,\; \forall j < l)$, then we have the partial likelihood
\begin{equation}
    L^{0l}(\Tilde{S}_T) = \Big[\prod_{j=1}^{l-1}\Tilde{S}_T(v_j)^{c_j}\Big]\Tilde{f}_T(v_l)^{d_l}
\end{equation}
This partial likelihood implies that \underline{$\Tilde{S}_T$ should remain at $1$ until the first event occurs}, i.e. $\Tilde{S}_T(t) = 1, \forall t < v_l$.  To see this, suppose the condition is violated so that $\Tilde{S}_T(v_l^-) < 1$.  We can thus define another survival function $\Tilde{S}^*_T$ by 
\begin{align}
    \Tilde{S}^*_T(t) = 1&, \quad t < v_l\\
    \Tilde{S}^*_T(t) = \Tilde{S}_T(t)&, \quad \text{otherwise}
\end{align}
Based on the definition, we have the following equalities and inequalities, where $0 < j < l$:
\begin{gather}
    \Tilde{S}^*_T(v_j) = 1 \ge \Tilde{S}_T(v_j)\\
    \Tilde{f}^*_T(v_l) = \Tilde{S}^*_T(v_l^-) - \Tilde{S}^*_T(v_l) = 1 - \Tilde{S}_T(v_l) > \Tilde{S}_T(v_l^-) - \Tilde{S}_T(v_l) =  \Tilde{f}_T(v_l)
\end{gather}
Therefore, we have
\begin{equation}
    L^{0l}(\Tilde{S}_T^*) = \Big[\prod_{j=1}^{l-1}\Tilde{S}^*_T(v_j)^{c_j}\Big]\Tilde{f}^*_T(v_l)^{d_l} > \Big[\prod_{j=1}^{l-1}\Tilde{S}_T(v_j)^{c_j}\Big]\Tilde{f}_T(v_l)^{d_l} = L^{01}(\Tilde{S}_T)
\end{equation}
which contradicts with the fact that $\Tilde{S}_T$ maximizes the partial likelihood $L^{0l}$.  

\medskip
At last, we set $l = m+1$ and let $k$ be the last timepoint with event occurrene, i.e. $(d_k \ne 0) \wedge (d_j = 0, \; \forall j > k)$, then we have the partial likelihood:
\begin{equation}
    L^{k(m+1)}(\Tilde{S}_T) = \Tilde{f}_T(v_k)^{d_k}\Big[\prod_{j=k}^{m}\Tilde{S}_T(v_j)^{c_j}\Big]
\end{equation}
This partial likelihood implies that \underline{$\Tilde{S}_T$ should not drop after the last event until the end of follow-up}, i.e. $\Tilde{S}_T(t) = \Tilde{S}_T(v_k), \; \forall t,  v_k < t \le v_m$. To see this, suppose the condition is violated so that $\Tilde{S}_T(v_m) < \Tilde{S}_T(v_k)$ where $v_k < v_m$. We thus may define another survival function $\Tilde{S}^*_T$ by
\begin{align}
    \Tilde{S}^*_T(t) = \Tilde{S}_T(v_k)&, \quad v_k < t \le v_m\\
    \Tilde{S}^*_T(t) = \Tilde{S}_T(t)&, \quad \text{otherwise}
\end{align}
Based on the definition, we have the following equalities and inequalities, where $k < j < m$:
\begin{gather}
    \Tilde{S}^*_T(v_k) = \Tilde{S}_T(v_k)\\
    \Tilde{f}^*_T(v_k) = \Tilde{S}^*_T(v_k^-) - \Tilde{S}^*_T(v_k) = \Tilde{S}_T(v_k^-) - \Tilde{S}_T(v_k) = \Tilde{f}_T(v_k)\\
    \Tilde{S}^*_T(v_j) = \Tilde{S}_T(v_k) \ge \Tilde{S}_T(v_j)\\
    \Tilde{S}^*_T(v_m) = \Tilde{S}_T(v_k) > \Tilde{S}_T(v_m)\\
\end{gather}
Therefore, we have
\begin{equation}
    L^{k(m+1)}(\Tilde{S}_T^*) = \Tilde{f}^*_T(v_k)^{d_k}\Big[\prod_{j=k}^{m}\Tilde{S}^*_T(v_j)^{c_j}\Big] > \Tilde{f}_T(v_k)^{d_k}\Big[\prod_{j=k}^{m}\Tilde{S}_T(v_j)^{c_j}\Big] = L^{k(m+1)}(\Tilde{S}_T)
\end{equation}
which contradicts with the fact that $\Tilde{S}_T$ maximizes the partial likelihood $L^{k(m+1)}$.  

\medskip
From the three observations above, we see the NPMLE for $S_T$ is a step function that can only drop at timepoints with event occurence (A technicality here is that when there are subjects censored at $v_m$, i.e. at the longest time of follow-up, the survival function \textit{can} drop at $t > v_m$ without affecting the likelihood.  In fact, in this case the NPMLE for $S_T$ is only identified up until $v_m$, and any valid survival function after $v_m$ does not influence the likelihood).  Therefore, we can reduce our NPMLE problem to a MLE problem with finite number of parameters. 
 Let $t_1, t_2..., t_q$ be the ordered timepoints where events occured, then we can reparameterize $S_T$ as 
\begin{equation}
    S_T(t) = \prod_{j, \;t_j \le t}(1-h_j) \label{eq: MLE_KM}
\end{equation}
where $0 \le h_j \le 1$, so that the survival function only drops at timepoints where events occured.

To find the MLE for the parameterization in \Cref{eq: MLE_KM} heuristically, note that the survival function can also be decomposed as follows, denoting $r$ as the largest integer satisfying $t_r \le t$:
\begin{align}
  S_T(t) &= \P(T > t)\\
  &= \P(T > t | T > t_r) \P(T > t_r)\\
  &= \P(T > t | T > t_r) \P(T > t_r | T > t_{r-1})\P(T > t_{r-1})\\
  & \qquad\qquad\qquad\qquad\qquad \vdots\\
  &= \P(T > t | T > t_r) \prod_{j=1}^r \P(T > t_j | T > t_{j-1}) \label{eq: MLE_KM_est}
\end{align}
The estimated $\P(T > t | T > t_r)$ would be $1$ since empirically there are no events within $(t_r,t]$.  Comparing \Cref{eq: MLE_KM,eq: MLE_KM_est}, we have the following relation
\begin{align}
    h_j &= 1-\P(T>t_j|T>t_{j-1})\\
    &= \P(T \le t_j|T>t_{j-1})\\
    &= \P(T \le t_j|T \ge t_j,T > t_{j-1})\P(T \ge t_j, t_{j-1})\\
    &= \P(T \le t_j|T \ge t_j) \label{eq: KM_explain}\\
    &= \P(T = t_j|T \ge t_j)
\end{align}
where in \Cref{eq: KM_explain}, since empirically there are no events within $(t_{j-1}, t_j]$, $T > t_{j-1}$ is equivalent to 
$T \ge t_j$.  In turn, $h_j := \P(T = t_j | T \ge t_j)$ can be estimated empirically by $\frac{d_j}{R_j}$, where $R_j$ is the number of subjects at risk (still in the study) right before $t_j$, and $d_j$ is the number of subjects experiencing event at time $t_j$.  This is called the \textbf{Kaplan-Meier estimator}.  To see how Kaplan-Meier estimators are calculated, we suppose that ten patients are followed up for events, and their observed event or censor times are (in month) $1, 1, 4^+, 6, 6^+, 8, 17, 17, 17, 20^+$ ($t^+$ implies censored at time $t$).  We would then have the following life table.
\begin{table}[ht]
    \centering
    \begin{tabular}{cccccc}
        $j$ & $t_j$ & $R_j$ & $d_j$ & $\hat{h}_j$ & $\hat{S}_j$\\
        \hline
        $1$ & $1$ & $10$ & $2$ & $2/10$ & $1 \times (1-2/10) = 0.8$\\
        $2$ & $6$ & $7$ & $1$ & $1/7$ & $0.8 \times (1-1/7) = 0.69$\\
        $3$ & $8$ & $5$ & $1$ & $1/5$ & $0.69 \times (1-1/5) = 0.55$\\
        $4$ & $17$ & $4$ & $3$ & $3/4$ & $0.55 \times (1-3/4) = 0.14$
    \end{tabular}
\end{table}

This estimated curve can be plotted using R as follows:

<<fig.width=4, fig.height=4, fig.align='center'>>=
survtime <- c(1,1,4,6,6,8,17,17,17,20)
event <- c(1,1,0,1,0,1,1,1,1,0)
km <- survfit(Surv(survtime, event) ~ 1)
plot(km)
@

The dotted line in the figure is the pointwise $95\%$ confidence interval for the curve.  To derive the formulae for the confidence intervals, first note the Kaplan-Meier estimator is
\begin{equation}
  \hat{S}_T(t) = \prod_{j,\; t_j \le t} (1-\hat{h}_j)
\end{equation}
Working with products is more difficult than working with sums, so we take the natural logarithm on both sides and yield
\begin{equation}
  \log(\hat{S}_T(t)) = \sum_{j,\; t_j \le t} \log(1-\hat{h}_j)
\end{equation}
Now assuming $\hat{h}_j$ are approximately mutually independent and using the delta method, we have:
\begin{align}
  var[\log(\hat{S}_T(t))] &= var[\sum_{j, \; t_j \le t} \log(1-\hat{h}_j)]\\
  &\approx \sum_{j, \; t_j \le t} var[\log(1-\hat{h}_j)]\\
  &\approx \sum_{j, \; t_j \le t} var[\hat{h}_j] \frac{1}{(1-\hat{h}_j)^2}
\end{align}
Now since $\hat{h}_j$ is an empirical proportion with $R_j$ participants, we can approximate its variance by $\frac{h_j(1-h_j)}{R_j} \approx \frac{\hat{h}_j(1-\hat{h}_j)}{R_j}$.  So we have 
\begin{equation}
  var[\log(\hat{S}_T(t))] \approx \sum_{j, \; t_j < t} \frac{\hat{h}_j(1-\hat{h}_j)}{R_j} \frac{1}{(1-\hat{h}_j)^2} = \sum_{j, \; t_j < t} \frac{d_j}{R_j(R_j-d_j)} \label{eq: logKM_var}
\end{equation}
Therefore, using the delta method again, we have the \textbf{Greenwood's formula}
\begin{equation}
  var[\hat{S}_T(t)] \approx var[\log\hat{S}_T(t)](\hat{S}_T(t))^2 \approx (\hat{S}_T(t))^2 \sum_{j, \; t_j < t} \frac{d_j}{R_j(R_j-d_j)}
\end{equation}
and the (pointwise) asymptotmic $(1-\alpha)$ confidence interval for $S_T(t)$ can be constructed as 
\begin{equation}
    \Bigg(\hat{S}_T(t) \pm Z_{1-\alpha/2} \hat{S}_T(t) \sqrt{\sum_{j, \; t_j < t} \frac{d_j}{R_j(R_j-d_j)}}\Bigg) \label{eq: Greenwood_suboptimal}
\end{equation}

\medskip
Eminent students may notice that \Cref{eq: Greenwood_suboptimal} can produce a confidence interval that is outside of $[0, 1]$ (especially when the number of events is small), which is unreasonable since survival functions should be bound between $0$ and $1$.  A commonly used strategy is to construct the confidence interval for $\log(-\log(S_T(t)))$ first since the function $\log(-\log(.))$ is bijective and maps $[0,1]$ to the real number line (and $\pm \infty$).  From \Cref{eq: logKM_var} using delta method again, we have
\begin{equation}
    var[\log(-\log(\hat{S}_T(t)))] \approx var[\log(\hat{S}_T(t))] \Big[\frac{-1}{\log(\hat{S}_T(t))}\Big]^2 \approx \frac{1}{[\log(\hat{S}_T(t))]^2} \sum_{j, \; t_j < t} \frac{d_j}{R_j(R_j-d_j)}
\end{equation}
So the (pointwise) asymptotic $(1-\alpha)$ confidence interval for $\log(-\log(S_T(t)))$ is
\begin{equation}
    \Bigg(\log(-\log(\hat{S}_T(t))) \mp Z_{1-\alpha/2} \frac{1}{\log(\hat{S}_T(t))} \sqrt{\sum_{j, \; t_j < t} \frac{d_j}{R_j(R_j-d_j)}}\Bigg)
\end{equation}
which, after tranformation back to $S_T(t)$, has the following confidence interval
\begin{equation}
    \Bigg([\hat{S}_T(t)]^{\exp\Big(\pm Z_{1-\alpha/2} \frac{1}{\log(\hat{S}_T(t))} \sqrt{\sum_{j, \; t_j < t} \frac{d_j}{R_j(R_j-d_j)}}\Big)} \Bigg)
\end{equation}

\subsection{Log-rank test and its derivatives}

Now we have shown how to estimate survival curves non-parametrically, the next question would be how to compare the survival curves between two (or more) groups.  That is, we would like to test for the null hypothesis $H_0: S_0(t) = S_1(t)$, where $S_0(t)$ and $S_1(t)$ are the survival functions for group $0$ and group $1$.  In parametric survival analyses, if the two survival curves belong to the same parametric family, then testing for equivalence of survival curves equates to setting their parameters to be identical, which can be tested using likelihood ratio tests.  However, in non-parameteric survival analyses, although we've shown that the Kaplan-Meier estimator is a non-parametric MLE, since the number of parameters for the whole survival function is in principle infinite, regular theories for tests against MLE may not work.

\medskip
One simple alternative is to test for equality of survival function at one timepoint or a finite set of timepoints.  For example, we may opt to test if $S_0(1) = S_1(1)$, which may be done by comparing $\hat{S}_0(1)$ and $\hat{S}_1(1)$, using Greenwood's formula to quantify their uncertainties.  However, this approach discards information from other timepoints, and can be of low power if $S_0$ and $S_1$ are similar near $t = 1$ but vastly different at other timepoints (e.g. scenarios (b) and (c) in the following figure).  In the following, we will introduce a test that truly tests the equivalence of the whole survival curve, which is the \textbf{log-rank test}.

\begin{figure}[ht]
    \centering
    \includegraphics[width = 0.7\textwidth]{survival_sections/graphs/survival curve.png}
\end{figure}

\medskip
We first define some additional notations.  Let $t_1, t_2, ..., t_q$ still be the ordered timepoints with event occurrence.  Denote $R_{0j}$ and $R_{1j}$ as the number of subjects at risk for group $0$ and group $1$ right before $t_j$, and write $R_j = R_{0j} + R_{1j}$ as the total number of at-risk subjects.  Denote $d_{0j}$ and $d_{1j}$ as the number of events occurring at $t_j$ in group $0$ and group $1$, and write $d_j = d_{0j} + d_{1j}$ as the total number of events.  Then for each timepoint $t_j$, we may construct the following table:

\begin{table}[ht]
    \centering
    \begin{tabular}{c|cc|c}
        $t=t_j$ & Event & Non-event & At risk \\
        \hline
        Group $0$ & $d_{0j}$ & $R_{0j} - d_{0j}$ & $R_{0j}$\\
        Group $1$ & $d_{1j}$ & $R_{1j} - d_{1j}$ & $R_{1j}$ \\
        \hline
        Total & $d_j$ & $R_j - d_j$ & $R_j$
    \end{tabular}
\end{table}

When the marginals $R_{0j}, R_{1j}, d_j, (R_j-d_j)$ treated as fixed, the cells in the table only have one degree of freedom, and let's set our cell of interest to be $d_{1j}$.  Under the null hypothesis of $H_0: S_0(t) = S_1(t)$, which is equivalent to $h_0(t) = h_1(t)$ and roughly means that "the subjects in both groups have the same probability of experiencing events", we can see this table as drawing $d_j$ balls from a pool of $R_{0j}$ "Group $0$ balls" and $R_{1j}$ "Group $1$ balls".  $d_{1j}$ is then the number of drawn "Group $1$ balls".  Therefore, the conditional distribution for $d_{1j}$ is a \textit{hypergeometric distribution} with parameters $N = R_j, K = R_{1j}$ and $n = d_j$.  Consequently, we can define
\begin{align}
    O_j &:= d_{1j}\\
    E_j &:= \E[O_j | R_j, R_{1j}, d_j] \\
    &= d_j \frac{R_{1j}}{R_j}\\
    V_j &:= var[O_j | R_j, R_{1j}, d_j] \\
    &= \frac{R_{1j}(R_j - R_{1j})d_j(R_j-d_j)}{R_j^2(R_j - 1)}
\end{align}
Now we can define the log-rank statistic by
\begin{gather}
    O := \sum_j O_j, \quad E := \sum_j E_j, \quad V := \sum_j V_j\\
    Z := \frac{O-E}{\sqrt{V}}
\end{gather}
which is approximately normally distributed under the null hypothesis.  

\medskip
An immediate observation for $Z$ is that when the hazards of group $1$ is always greater than group $0$, then $O$, which is the total number of events that occurred in group $1$, would be greater than expected, leading to a large positive $Z$.  In contrasts, when the hazards of group $1$ is always lesser than group $0$, we would get a negative $Z$ with large magnitude.  However, in the case where the hazards of the two groups are intertwined like scenario (c) in the figure above, $O$ may not be too different than $E$ since $O_j$ is sometimes larger than and sometime smaller than $E_j$.  In fact, the log-rank test can be reformulated as testing if $\theta = 1$ in the hypothesis $h_1(t) = \theta h_0(t)$.  Therefore, it has the best power when the hazards are truly proportional like scenario (a), sub-optimal power when the hazards of one group is still greater than the other across time but not proportional like scenario (b), and has \textbf{low power if the hazard functions of the two groups cross} like scenario (c). 

\medskip
From the discussion above, we see that the vanilla log-rank test statistic treats each contingency table as equally important, so it may have sub-optimal power when then difference in hazards is greater in some time intervals and lesser in other.  In light of this, based on prior knowledge for when the hazards difference would be greater, we can construct \textbf{weighted log-rank test} statistics as follows
\begin{equation}
    Z(\boldsymbol{w}) = \frac{\sum_j w_j(O_j - E_j)}{\sqrt{\sum_j w_j^2V_j}}
\end{equation}
where the weight $w_j$ is set to be larger at timepoints that is expected have larger hazards difference.  The null distribution of this statistic is still approximately normal. Some common settings for the weights are as follows:

\begin{table}[ht]
    \centering
    \begin{tabular}{cc}
        Name & $w_j$ \\
        Log-rank & $1$\\
        Wilcoxon & $R_j$\\
        Tarone-Ware & $\sqrt{R_j}$\\
        Peto-Prentice & $\hat{S}(t_j)$\\
        Fleming-Harrington & $[\hat{S}(t_{j-1})]^p[1-\hat{S}(t_{j-1})]^q$
    \end{tabular}
\end{table}

where $\hat{S}(t)$ is the Kaplan-Meier estimator for both groups combined.  Notice taht Wilcoxon, Taron-Ware and Peto-Prentice all place larger weights at earlier timepoints, while for Fleming-Harrington we may place larger weights at later timepoints by setting a larger $q$.

\medskip
Another extension to the log-rank test is to account for between-group difference in characteristics.  For example, suppose we want to know if the treated group has better survival than the untreated group, but we observe that the treated group has younger patients, who naturally have better survival even without the treatment.  In this case, we may split the data into the $L = 2$ strata consisting of young patients ($l = 0$) and old patients ($l = 1$).  For each stratum $l \in \{0, 1\}$ we yield $O^{(l)}, E^{(l)}$ as the observed and expected number of events, and $V^{(l)}$ as the variance for $O^{(l)}$.  We can then construct the \textbf{stratified log-rank test} statistic as
\begin{equation}
    Z_{\text{strat}} = \frac{\sum_l (O^{(l)}-E^{(l)})}{\sqrt{\sum_l V^{(l)}}}
\end{equation}
Note that now with each strata, all patients are of the same age group, so the difference between $O^{(l)}$ and $E^{(l)}$ cannot stem from difference in age distribution between groups, and can thus be better ascribed to the effect of the treatment.

\medskip
The last extension to log-rank test we'll talk about is the case where there are more than two groups, say, $G+1$ groups labeled $\{0, 1, 2,...,G\}$.  In this case, the null hypothesis we are testing would be $H_0: S_0(t) = S_1(t) = S_2(t) = ... = S_G(t)$, and the contingency table for timepoint $t_j$ would shape as follows

\begin{table}[ht]
    \centering
    \begin{tabular}{c|cc|c}
        $t=t_j$ & Event & Non-event & At risk \\
        \hline
        Group $0$ & $d_{0j}$ & $R_{0j} - d_{0j}$ & $R_{0j}$\\
        Group $1$ & $d_{1j}$ & $R_{1j} - d_{1j}$ & $R_{1j}$\\
        Group $2$ & $d_{2j}$ & $R_{2j} - d_{2j}$ & $R_{2j}$\\
        \vdots & \vdots & \vdots & \vdots\\
        Group $G$ & $d_{Gj}$ & $R_{Gj} - d_{Gj}$ & $R_{Gj}$\\
        \hline
        Total & $d_j$ & $R_j - d_j$ & $R_j$
    \end{tabular}
\end{table}

Now the degrees of freedom for the cells after fixing the marginals would be $G$, and we may set $d_{1j}, d_{2j}, ..., d_{Gj}$ as the cells of interest, whose condition joint distribution would be a \textit{multivariate hypergeometric distribution} under the null hypothesis.  In light of this, we can define the follows
\begin{align}
    O_j &:= (d_{1j}, d_{2j}, ..., d_{Gj})^\top\\
    E_j &:= \E[O_j|R_j, d_j, R_{1j}, R_{2j}, ..., R_{Gj}]\\
    &= \frac{d_j}{R_j}(R_{1j}, R_{2j}, ..., R_{Gj})^\top\\
    V_j &:= var[O_j|R_j, d_j, R_{1j}, R_{2j}, ..., R_{Gj}]
\end{align}
where for the detail of the matrix $V_j$:
\begin{gather}
    V_{j(kk)} = \frac{R_{kj}(R_j - R_{kj})d_j(R_j-d_j)}{R_j^2(R_j - 1)}\\
    V_{j(kl)} = -\frac{R_{kj}R_{lj}d_j(R_j-d_j)}{R_j^2(R_j - 1)}
\end{gather}
Now we can define the log-rank statistic by
\begin{gather}
    O := \sum_j O_j, \quad E := \sum_j E_j, \quad V := \sum_j V_j\\
    Z := (O-E)^\top V^{-1} (O-E)
\end{gather}
which is approximately distributed as $\chi^2_G$ under the null hypothesis.  Note that this test, like ANOVA, tests if the survival function of \textit{any} group deviates from others.  To clarify which group is different from which, we will still need post-hoc tests.